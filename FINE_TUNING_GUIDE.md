# RAG Chatbot Fine-Tuning Guide

This guide explains how to fine-tune your RAG chatbot using Google Colab for improved performance on your specific documents.

## üéØ Overview

The fine-tuning component includes:
- **Data preparation**: Converts documents to training format
- **Google Colab integration**: Pre-built notebook for GPU training
- **Model switching**: Seamlessly switch between base and fine-tuned models
- **Configuration management**: Customizable training parameters

## üöÄ Quick Start

### 1. Prepare Fine-Tuning Data

```python
from rag_chat import ConfigurableDocBot

# Initialize bot and load domain
bot = ConfigurableDocBot()
bot.load_domain("medical", "./Documents/Medical")

# Prepare fine-tuning data
results = bot.prepare_fine_tuning_data()
print(f"Created files: {results}")
```

This creates:
- `fine_tuning_data/instruction_data_*.jsonl` - Training data
- `fine_tuning_colab.ipynb` - Google Colab notebook

### 2. Fine-Tune in Google Colab

1. Upload `fine_tuning_colab.ipynb` to [Google Colab](https://colab.research.google.com/)
2. Upload your training data file when prompted
3. Run all cells to start training
4. Download the fine-tuned model

### 3. Use Fine-Tuned Model

```python
# Initialize with fine-tuned model
bot = ConfigurableDocBot(
    use_fine_tuned=True,
    fine_tuned_model_path="./rag_chatbot_fine_tuned"
)

# Or switch dynamically
bot.switch_to_fine_tuned_model("./rag_chatbot_fine_tuned")
```

## üìã Interactive Commands

When running `rag_chat.py`, you can use these commands:

- **`fine-tune`**: Prepare fine-tuning data from current documents
- **`use-fine-tuned`**: Switch to a fine-tuned model
- **`switch`**: Change document domains

## ‚öôÔ∏è Configuration

Edit `fine_tuning_config.py` to customize:

### Training Parameters
```python
LEARNING_RATE = 5e-5      # Learning rate
BATCH_SIZE = 4            # Batch size
NUM_EPOCHS = 3            # Training epochs
```

### Model Configuration
```python
BASE_MODEL = "microsoft/DialoGPT-medium"
LORA_RANK = 16            # LoRA rank for parameter-efficient fine-tuning
```

### Data Processing
```python
CHUNK_SIZE = 1000         # Document chunk size
MAX_TRAINING_EXAMPLES = 1000  # Max training examples
```

## üìä Training Data Formats

### Instruction Format (Default)
```json
{
  "instruction": "Explain the following content:",
  "input": "Document chunk here...",
  "output": "Generated explanation..."
}
```

### QA Format
```json
{
  "question": "What is aspirin used for?",
  "context": "Aspirin is used for pain relief...",
  "answer": "Aspirin is used for pain relief, fever reduction..."
}
```

### Chat Format
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant..."},
    {"role": "user", "content": "Question about document..."},
    {"role": "assistant", "content": "Answer based on document..."}
  ]
}
```

## üéõÔ∏è Advanced Features

### Custom QA Pairs
Prepare your own QA pairs for supervised fine-tuning:

```python
qa_pairs = [
    {
        "question": "What are the side effects of aspirin?",
        "answer": "Common side effects include stomach upset...",
        "source": "medical_document.pdf"
    }
]

results = bot.prepare_fine_tuning_data(qa_pairs=qa_pairs)
```

### Model Providers
The fine-tuned model can be used with:
- **Ollama**: Local inference server
- **HuggingFace**: Direct model loading
- **Transformers**: PyTorch integration

### Parameter-Efficient Fine-Tuning
Uses LoRA (Low-Rank Adaptation) for:
- 95% fewer trainable parameters
- Faster training
- Lower memory usage
- Comparable quality

## üîß Troubleshooting

### Common Issues

**Model loading fails:**
```bash
# Check model path exists
ls -la ./rag_chatbot_fine_tuned

# Verify Ollama has access
ollama list
```

**Training data issues:**
```bash
# Validate JSON format
python -c "import json; json.load(open('training_data.jsonl'))"
```

**Colab errors:**
- Ensure GPU runtime is enabled
- Check training data upload completed
- Monitor memory usage

### Performance Tips

1. **Document Quality**: Clean, well-structured documents perform better
2. **Training Examples**: More diverse QA pairs improve generalization
3. **Model Selection**: Start with smaller base models for faster iteration
4. **Hyperparameters**: Tune learning rate and batch size for your dataset

## üìö Best Practices

### Data Preparation
- Use domain-specific documents
- Generate diverse question types
- Include context in training examples
- Balance question difficulty

### Model Training
- Start with small epochs (2-3)
- Use validation split if possible
- Monitor training loss
- Save checkpoints regularly

### Integration
- Test fine-tuned model on unseen questions
- Compare performance with base model
- Keep backup of working models
- Document model versions

## üéØ Example Workflow

```python
# 1. Load domain and prepare data
bot = ConfigurableDocBot()
bot.load_domain("legal", "./Documents/Legal")
results = bot.prepare_fine_tuning_data()

# 2. Fine-tune in Colab (manual step)
# Upload notebook and data, run training, download model

# 3. Switch to fine-tuned model
bot.switch_to_fine_tuned_model("./rag_chatbot_fine_tuned")

# 4. Test improved performance
answer = bot.ask("What are the contract termination clauses?")
print(answer)
```

## üìà Expected Improvements

Fine-tuned models typically show:
- **20-40%** better answer accuracy
- **Improved domain knowledge**
- **Better context understanding**
- **More consistent responses**

## üîÑ Model Management

Track fine-tuned models with clear naming:
```
models/
‚îú‚îÄ‚îÄ base_cus-qwen/
‚îú‚îÄ‚îÄ medical_finetuned_v1/
‚îú‚îÄ‚îÄ legal_finetuned_v2/
‚îî‚îÄ‚îÄ tech_finetuned_v1/
```

Use semantic versioning for model updates:
- `v1.0.0` - Initial fine-tune
- `v1.1.0` - Improved data
- `v2.0.0` - New base model

## üöÄ Next Steps

1. **Experiment**: Try different base models and parameters
2. **Evaluate**: Compare fine-tuned vs base performance
3. **Deploy**: Integrate into production workflows
4. **Monitor**: Track real-world performance
5. **Iterate**: Continuously improve with new data

For more technical details, see the inline code documentation in `fine_tuning.py` and `rag_chat.py`.