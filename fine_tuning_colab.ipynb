{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ü§ñ Configurable LLM Chatbot with Fine-Tuning\n",
        "\n",
        "This notebook provides a complete RAG (Retrieval-Augmented Generation) chatbot with fine-tuning capabilities for custom models.\n",
        "\n",
        "## Features:\n",
        "- üìö Document loading (PDF, TXT, MD)\n",
        "- üîç Vector search with ChromaDB\n",
        "- üéØ Custom fine-tuning of language models\n",
        "- üí¨ Interactive Q&A interface\n",
        "- üîÑ Domain switching\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run all cells in order\n",
        "2. Upload your documents to the `Documents/` folder\n",
        "3. Configure your domain and fine-tune if needed\n",
        "4. Start chatting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üõ†Ô∏è Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q langchain langchain-core langchain-community langchain-text-splitters\n",
        "!pip install -q langchain-ollama langchain-chroma langchain-huggingface\n",
        "!pip install -q chromadb sentence-transformers pypdf pandas\n",
        "!pip install -q transformers datasets torch accelerate bitsandbytes\n",
        "!pip install -q peft trl wandb\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports completed!\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import torch\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_ollama import OllamaLLM \n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Fine-tuning imports\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer,\n",
        "    BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "import wandb\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"‚úÖ All imports completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## ‚öôÔ∏è Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "config_setup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Configuration loaded:\n",
            "  Base Model: microsoft/DialoGPT-medium\n",
            "  Ollama Model: chatter\n",
            "  Use LoRA: True\n",
            "  Batch Size: 4\n",
            "  Learning Rate: 0.0002\n"
          ]
        }
      ],
      "source": [
        "# Configuration class\n",
        "class Config:\n",
        "    # Model settings\n",
        "    BASE_MODEL = \"microsoft/DialoGPT-medium\"  # Base model for fine-tuning\n",
        "    OLLAMA_MODEL = \"chatter\"  # Ollama model for RAG\n",
        "    \n",
        "    # Fine-tuning settings\n",
        "    USE_LORA = True\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.1\n",
        "    \n",
        "    # Training settings\n",
        "    BATCH_SIZE = 4\n",
        "    GRAD_ACCUM_STEPS = 4\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NUM_EPOCHS = 3\n",
        "    MAX_LENGTH = 512\n",
        "    \n",
        "    # RAG settings\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "    RETRIEVAL_K = 4\n",
        "    \n",
        "    # Paths\n",
        "    DOCS_FOLDER = \"./Documents\"\n",
        "    FINE_TUNED_MODEL_PATH = \"./fine_tuned_model\"\n",
        "    CHROMA_DB_PATH = \"./chroma_db\"\n",
        "    \n",
        "# Initialize config\n",
        "config = Config()\n",
        "print(f\"üìã Configuration loaded:\")\n",
        "print(f\"  Base Model: {config.BASE_MODEL}\")\n",
        "print(f\"  Ollama Model: {config.OLLAMA_MODEL}\")\n",
        "print(f\"  Use LoRA: {config.USE_LORA}\")\n",
        "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Learning Rate: {config.LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fine_tuning"
      },
      "source": [
        "## üéØ Fine-Tuning Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fine_tuning_class"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fine-tuning component defined!\n"
          ]
        }
      ],
      "source": [
        "class ModelFineTuner:\n",
        "    \"\"\"Fine-tuning component for custom language models\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "        \n",
        "    def setup_model(self):\n",
        "        \"\"\"Setup model and tokenizer for fine-tuning\"\"\"\n",
        "        print(\"üîß Setting up model for fine-tuning...\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.config.BASE_MODEL,\n",
        "            padding_side=\"right\",\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        # Quantization config (for memory efficiency)\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "        )\n",
        "        \n",
        "        # Load model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.config.BASE_MODEL,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        # Setup LoRA if enabled\n",
        "        if self.config.USE_LORA:\n",
        "            self._setup_lora()\n",
        "            \n",
        "        print(\"‚úÖ Model setup completed!\")\n",
        "        \n",
        "    def _setup_lora(self):\n",
        "        \"\"\"Setup LoRA for parameter-efficient fine-tuning\"\"\"\n",
        "        lora_config = LoraConfig(\n",
        "            r=self.config.LORA_R,\n",
        "            lora_alpha=self.config.LORA_ALPHA,\n",
        "            target_modules=[\"c_attn\", \"c_proj\"],\n",
        "            lora_dropout=self.config.LORA_DROPOUT,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.CAUSAL_LM\n",
        "        )\n",
        "        \n",
        "        self.peft_model = get_peft_model(self.model, lora_config)\n",
        "        self.peft_model.print_trainable_parameters()\n",
        "        print(\"‚úÖ LoRA setup completed!\")\n",
        "        \n",
        "    def prepare_dataset(self, conversations: List[Dict]) -> Dataset:\n",
        "        \"\"\"Prepare dataset for fine-tuning\"\"\"\n",
        "        print(\"üìä Preparing dataset...\")\n",
        "        \n",
        "        def format_conversation(conv):\n",
        "            \"\"\"Format conversation for training\"\"\"\n",
        "            if isinstance(conv, dict) and 'messages' in conv:\n",
        "                # Chat format\n",
        "                formatted = \"\"\n",
        "                for msg in conv['messages']:\n",
        "                    role = msg.get('role', 'user')\n",
        "                    content = msg.get('content', '')\n",
        "                    formatted += f\"{role}: {content}\\n\"\n",
        "                return formatted + \"assistant: \"\n",
        "            else:\n",
        "                # Simple Q&A format\n",
        "                return f\"user: {conv.get('question', '')}\\nassistant: {conv.get('answer', '')}\"\n",
        "        \n",
        "        # Format and tokenize\n",
        "        formatted_texts = [format_conversation(conv) for conv in conversations]\n",
        "        \n",
        "        dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
        "        \n",
        "        def tokenize_function(examples):\n",
        "            return self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=self.config.MAX_LENGTH,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "        \n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "        print(f\"‚úÖ Dataset prepared with {len(tokenized_dataset)} examples\")\n",
        "        \n",
        "        return tokenized_dataset\n",
        "        \n",
        "    def fine_tune(self, dataset: Dataset, save_path: str = None):\n",
        "        \"\"\"Fine-tune the model\"\"\"\n",
        "        print(\"üöÄ Starting fine-tuning...\")\n",
        "        \n",
        "        if save_path is None:\n",
        "            save_path = self.config.FINE_TUNED_MODEL_PATH\n",
        "            \n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=save_path,\n",
        "            num_train_epochs=self.config.NUM_EPOCHS,\n",
        "            per_device_train_batch_size=self.config.BATCH_SIZE,\n",
        "            gradient_accumulation_steps=self.config.GRAD_ACCUM_STEPS,\n",
        "            learning_rate=self.config.LEARNING_RATE,\n",
        "            fp16=True,\n",
        "            logging_steps=10,\n",
        "            save_steps=100,\n",
        "            save_total_limit=2,\n",
        "            evaluation_strategy=\"no\",\n",
        "            load_best_model_at_end=False,\n",
        "            report_to=\"none\",  # Disable wandb logging unless needed\n",
        "        )\n",
        "        \n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "        \n",
        "        # Trainer\n",
        "        model_to_train = self.peft_model if self.peft_model else self.model\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=model_to_train,\n",
        "            args=training_args,\n",
        "            train_dataset=dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "        \n",
        "        # Start training\n",
        "        trainer.train()\n",
        "        \n",
        "        # Save model\n",
        "        trainer.save_model(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "        \n",
        "        print(f\"‚úÖ Fine-tuning completed! Model saved to {save_path}\")\n",
        "        \n",
        "    def load_fine_tuned_model(self, model_path: str):\n",
        "        \"\"\"Load a fine-tuned model\"\"\"\n",
        "        print(f\"üìÇ Loading fine-tuned model from {model_path}...\")\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        \n",
        "        if self.config.USE_LORA:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.config.BASE_MODEL,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "            self.peft_model = get_peft_model(self.model, LoraConfig(\n",
        "                r=self.config.LORA_R,\n",
        "                lora_alpha=self.config.LORA_ALPHA,\n",
        "                target_modules=[\"c_attn\", \"c_proj\"],\n",
        "                lora_dropout=self.config.LORA_DROPOUT,\n",
        "                bias=\"none\",\n",
        "                task_type=TaskType.CAUSAL_LM\n",
        "            ))\n",
        "            \n",
        "            # Load LoRA weights\n",
        "            from peft import PeftModel\n",
        "            self.peft_model = PeftModel.from_pretrained(self.model, model_path)\n",
        "        else:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "            \n",
        "        print(\"‚úÖ Fine-tuned model loaded successfully!\")\n",
        "        \n",
        "    def generate_response(self, prompt: str, max_length: int = 100) -> str:\n",
        "        \"\"\"Generate response using fine-tuned model\"\"\"\n",
        "        if not self.tokenizer or not (self.model or self.peft_model):\n",
        "            return \"Model not loaded. Please setup or load a model first.\"\n",
        "            \n",
        "        model_to_use = self.peft_model if self.peft_model else self.model\n",
        "        \n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True).to(model_to_use.device)\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model_to_use.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Remove the original prompt\n",
        "        if response.startswith(prompt):\n",
        "            response = response[len(prompt):].strip()\n",
        "            \n",
        "        return response\n",
        "\n",
        "print(\"‚úÖ Fine-tuning component defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rag_component"
      },
      "source": [
        "## üìö RAG Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rag_class"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAG component defined!\n"
          ]
        }
      ],
      "source": [
        "class ConfigurableDocBot:\n",
        "    \"\"\"RAG-based document chatbot\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config, model_name: str = None):\n",
        "        \"\"\"Initialize the configurable RAG chatbot\"\"\"\n",
        "        self.config = config\n",
        "        self.model_name = model_name or config.OLLAMA_MODEL\n",
        "        \n",
        "        # Initialize components\n",
        "        self.llm = None\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.current_domain = None\n",
        "        \n",
        "        print(\"üöÄ Initializing Configurable Document Bot...\")\n",
        "        self._setup_embeddings()\n",
        "        self._setup_llm()\n",
        "        \n",
        "    def _setup_embeddings(self):\n",
        "        \"\"\"Setup embedding model\"\"\"\n",
        "        print(\"‚è≥ Loading embeddings model (first time downloads ~90MB)...\")\n",
        "        \n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=self.config.EMBEDDING_MODEL,\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "        print(\"‚úÖ Embeddings model loaded!\\n\")\n",
        "        \n",
        "    def _setup_llm(self):\n",
        "        \"\"\"Setup language model\"\"\"\n",
        "        try:\n",
        "            self.llm = OllamaLLM(model=self.model_name)\n",
        "            print(f\"‚úÖ Ollama LLM '{self.model_name}' loaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load Ollama LLM: {e}\")\n",
        "            print(\"üí° Make sure Ollama is installed and the model is available\")\n",
        "            \n",
        "    def load_domain(self, domain_name: str, docs_folder: str) -> bool:\n",
        "        \"\"\"Load documents for a specific domain/customer\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üìÇ Loading domain: {domain_name}\")\n",
        "        print(f\"üìÅ Document folder: {docs_folder}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        self.current_domain = domain_name\n",
        "        \n",
        "        # Clean up old vector store\n",
        "        db_path = f\"{self.config.CHROMA_DB_PATH}_{domain_name}\"\n",
        "        if os.path.exists(db_path):\n",
        "            print(f\"üóëÔ∏è Removing old vector database...\")\n",
        "            shutil.rmtree(db_path)\n",
        "        \n",
        "        # Load documents\n",
        "        documents = self._load_documents(docs_folder)\n",
        "        \n",
        "        if not documents:\n",
        "            print(f\"\\n‚ö†Ô∏è WARNING: No documents found in {docs_folder}\")\n",
        "            print(f\"‚ÑπÔ∏è The bot will respond that information is not available\")\n",
        "            self.vectorstore = None\n",
        "            self.qa_chain = None\n",
        "            return False\n",
        "        \n",
        "        # Process documents and create vector database\n",
        "        self._process_documents(documents, domain_name)\n",
        "        \n",
        "        # Setup QA chain\n",
        "        self._setup_qa_chain()\n",
        "        \n",
        "        print(f\"\\n‚úÖ Domain '{domain_name}' configured successfully!\")\n",
        "        print(f\"üìä Ready to answer questions about {len(documents)} documents\\n\")\n",
        "        return True\n",
        "        \n",
        "    def _load_documents(self, docs_folder: str) -> List[Document]:\n",
        "        \"\"\"Load all supported document types from folder\"\"\"\n",
        "        documents = []\n",
        "        folder_path = Path(docs_folder)\n",
        "        \n",
        "        if not folder_path.exists():\n",
        "            print(f\"‚ùå Error: Folder '{docs_folder}' does not exist\")\n",
        "            return documents\n",
        "        \n",
        "        print(\"\\nüìÑ Loading documents...\")\n",
        "        \n",
        "        # Load markdown and text files\n",
        "        for ext in ['*.md', '*.txt']:\n",
        "            for file_path in folder_path.rglob(ext):\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                        if content.strip():  # Only if not empty\n",
        "                            doc = Document(\n",
        "                                page_content=content,\n",
        "                                metadata={\"source\": str(file_path)}\n",
        "                            )\n",
        "                            documents.append(doc)\n",
        "                            print(f\"  ‚úì Loaded {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è Error loading {file_path.name}: {e}\")\n",
        "        \n",
        "        # Load PDF files\n",
        "        try:\n",
        "            from pypdf import PdfReader\n",
        "            for file_path in folder_path.rglob('*.pdf'):\n",
        "                try:\n",
        "                    reader = PdfReader(str(file_path))\n",
        "                    text = \"\"\n",
        "                    for page in reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "                    \n",
        "                    if text.strip():\n",
        "                        doc = Document(\n",
        "                            page_content=text,\n",
        "                            metadata={\"source\": str(file_path)}\n",
        "                        )\n",
        "                        documents.append(doc)\n",
        "                        print(f\"  ‚úì Loaded {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è Error loading {file_path.name}: {e}\")\n",
        "        except ImportError:\n",
        "            print(\"  ‚ÑπÔ∏è pypdf not available, skipping PDF files\")\n",
        "        \n",
        "        if documents:\n",
        "            print(f\"\\nüìö Total documents loaded: {len(documents)}\")\n",
        "        \n",
        "        return documents\n",
        "        \n",
        "    def _process_documents(self, documents: List[Document], domain_name: str):\n",
        "        \"\"\"Split documents into chunks and create vector database\"\"\"\n",
        "        print(\"\\n‚öôÔ∏è Processing documents...\")\n",
        "        \n",
        "        # Split documents into smaller chunks for better retrieval\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.CHUNK_SIZE,\n",
        "            chunk_overlap=self.config.CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "        )\n",
        "        \n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        print(f\"  ‚úì Split into {len(texts)} chunks\")\n",
        "        \n",
        "        # Create vector database\n",
        "        print(f\"  ‚è≥ Creating vector database...\")\n",
        "        self.vectorstore = Chroma.from_documents(\n",
        "            documents=texts,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=f\"{self.config.CHROMA_DB_PATH}_{domain_name}\"\n",
        "        )\n",
        "        print(f\"  ‚úì Vector database created and persisted\")\n",
        "        \n",
        "    def _setup_qa_chain(self):\n",
        "        \"\"\"Setup the question-answering chain\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            print(\"‚ö†Ô∏è No vector store - QA chain not created\")\n",
        "            return\n",
        "        \n",
        "        if self.llm is None:\n",
        "            print(\"‚ö†Ô∏è No LLM available - QA chain not created\")\n",
        "            return\n",
        "            \n",
        "        print(\"  ‚è≥ Setting up QA chain...\")\n",
        "        \n",
        "        # Create retriever (finds relevant chunks)\n",
        "        retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": self.config.RETRIEVAL_K}\n",
        "        )\n",
        "        \n",
        "        # Create prompt template\n",
        "        prompt_template = \"\"\"CONTEXT FROM DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer using ONLY the information in the CONTEXT above\n",
        "2. Follow a step-by-step format from your system prompt\n",
        "3. If the answer is not in the CONTEXT, respond: \"This information is not available to me. If you want, I can contact a human agent!\"\n",
        "4. Cite specific sections or sources when possible\n",
        "\n",
        "YOUR ANSWER:\"\"\"\n",
        "\n",
        "        PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "        \n",
        "        # Create the QA chain\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT}\n",
        "        )\n",
        "        \n",
        "        print(\"  ‚úì QA chain ready\")\n",
        "        \n",
        "    def ask(self, question: str) -> str:\n",
        "        \"\"\"Ask a question about the loaded documents\"\"\"\n",
        "        if not self.current_domain:\n",
        "            return \"‚ùå No domain loaded. Use load_domain() first.\"\n",
        "        \n",
        "        if self.qa_chain is None:\n",
        "            return \"This information is not available to me. If you want, I can contact a human agent!\"\n",
        "        \n",
        "        print(f\"\\n{'‚îÄ'*60}\")\n",
        "        print(f\"üîç Question: {question}\")\n",
        "        print(f\"üìÇ Domain: {self.current_domain}\")\n",
        "        print(\"‚è≥ Searching documents...\")\n",
        "        \n",
        "        try:\n",
        "            # Query the chain\n",
        "            result = self.qa_chain.invoke({\"query\": question})\n",
        "            \n",
        "            answer = result[\"result\"]\n",
        "            \n",
        "            # Display source documents\n",
        "            if result.get(\"source_documents\"):\n",
        "                print(f\"\\nüìÑ Sources used:\")\n",
        "                for i, doc in enumerate(result[\"source_documents\"][:3], 1):\n",
        "                    source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "                    snippet = doc.page_content[:150].replace(\"\\n\", \" \")\n",
        "                    print(f\"  {i}. {os.path.basename(source)}\")\n",
        "                    print(f\"     Preview: {snippet}...\")\n",
        "            \n",
        "            print(f\"{'‚îÄ'*60}\\n\")\n",
        "            return answer\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error: {str(e)}\\n\")\n",
        "            return f\"Error processing question: {str(e)}\"\n",
        "        \n",
        "    def switch_domain(self, domain_name: str, docs_folder: str) -> bool:\n",
        "        \"\"\"Switch to a different domain/customer\"\"\"\n",
        "        print(f\"\\nüîÑ Switching from '{self.current_domain}' to '{domain_name}'...\")\n",
        "        return self.load_domain(domain_name, docs_folder)\n",
        "\n",
        "print(\"‚úÖ RAG component defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_data"
      },
      "source": [
        "## üìù Sample Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "create_sample_docs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sample documents created in ./Documents\n"
          ]
        }
      ],
      "source": [
        "# Create sample documents for testing\n",
        "def create_sample_documents():\n",
        "    \"\"\"Create sample documents for demonstration\"\"\"\n",
        "    \n",
        "    # Create Documents directory\n",
        "    docs_dir = Path(config.DOCS_FOLDER)\n",
        "    docs_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create subdirectories\n",
        "    medical_dir = docs_dir / \"Medical\"\n",
        "    legal_dir = docs_dir / \"Legal\"\n",
        "    tech_dir = docs_dir / \"Tech\"\n",
        "    \n",
        "    for subdir in [medical_dir, legal_dir, tech_dir]:\n",
        "        subdir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Medical documents\n",
        "    medical_content = \"\"\"\n",
        "# Medical Guidelines\n",
        "\n",
        "## Common Medical Conditions\n",
        "\n",
        "### Hypertension\n",
        "Hypertension, or high blood pressure, is a common medical condition affecting millions worldwide. \n",
        "Normal blood pressure is typically around 120/80 mmHg. Hypertension is diagnosed when blood \n",
        "pressure consistently exceeds 130/80 mmHg.\n",
        "\n",
        "### Diabetes\n",
        "Diabetes mellitus is a group of metabolic disorders characterized by high blood sugar levels \n",
        "over a prolonged period. Type 2 diabetes is the most common form, accounting for 90-95% of cases.\n",
        "\n",
        "## Treatment Recommendations\n",
        "\n",
        "For hypertension patients, lifestyle modifications including:\n",
        "- Reduced sodium intake\n",
        "- Regular physical activity\n",
        "- Weight management\n",
        "- Stress reduction techniques\n",
        "\n",
        "For diabetes management:\n",
        "- Regular blood glucose monitoring\n",
        "- Dietary modifications\n",
        "- Physical activity\n",
        "- Medication as prescribed\n",
        "\"\"\"\n",
        "    \n",
        "    with open(medical_dir / \"guidelines.md\", \"w\") as f:\n",
        "        f.write(medical_content)\n",
        "    \n",
        "    # Legal documents\n",
        "    legal_content = \"\"\"\n",
        "# Legal Information\n",
        "\n",
        "## Contract Law Basics\n",
        "\n",
        "A contract is a legally binding agreement between two or more parties. For a contract to be \n",
        "valid, it must contain several essential elements:\n",
        "\n",
        "1. **Offer**: One party proposes to enter into an agreement\n",
        "2. **Acceptance**: The other party agrees to the terms\n",
        "3. **Consideration**: Something of value is exchanged\n",
        "4. **Capacity**: All parties have the legal ability to contract\n",
        "5. **Legality**: The contract's purpose is legal\n",
        "\n",
        "## Types of Contracts\n",
        "\n",
        "### Employment Contracts\n",
        "Employment contracts outline the terms of employment between an employer and employee. \n",
        "Key provisions include salary, work hours, responsibilities, and termination conditions.\n",
        "\n",
        "### Service Agreements\n",
        "Service agreements define the terms under which one party provides services to another. \n",
        "These should clearly specify scope of work, payment terms, and deliverables.\n",
        "\"\"\"\n",
        "    \n",
        "    with open(legal_dir / \"contracts.md\", \"w\") as f:\n",
        "        f.write(legal_content)\n",
        "    \n",
        "    # Tech documents\n",
        "    tech_content = \"\"\"\n",
        "# Technical Documentation\n",
        "\n",
        "## Python Programming\n",
        "\n",
        "Python is a high-level, interpreted programming language known for its simplicity and \n",
        "readability. It's widely used in web development, data science, machine learning, and automation.\n",
        "\n",
        "### Key Features\n",
        "- Dynamic typing\n",
        "- Automatic memory management\n",
        "- Extensive standard library\n",
        "- Cross-platform compatibility\n",
        "\n",
        "## Machine Learning Basics\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that enables systems to learn and \n",
        "improve from experience without being explicitly programmed.\n",
        "\n",
        "### Common Algorithms\n",
        "\n",
        "#### Linear Regression\n",
        "Linear regression is used for predicting continuous values. It models the relationship \n",
        "between dependent and independent variables using a linear approach.\n",
        "\n",
        "#### Decision Trees\n",
        "Decision trees are non-parametric supervised learning methods used for classification and \n",
        "regression. They work by learning simple decision rules inferred from data features.\n",
        "\n",
        "### Best Practices\n",
        "1. Data preprocessing and cleaning\n",
        "2. Feature engineering and selection\n",
        "3. Model validation using cross-validation\n",
        "4. Hyperparameter tuning\n",
        "5. Regular monitoring and retraining\n",
        "\"\"\"\n",
        "    \n",
        "    with open(tech_dir / \"programming.md\", \"w\") as f:\n",
        "        f.write(tech_content)\n",
        "    \n",
        "    print(f\"‚úÖ Sample documents created in {config.DOCS_FOLDER}\")\n",
        "\n",
        "# Create sample documents\n",
        "create_sample_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "create_training_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 5 training examples\n",
            "Sample training data:\n",
            "\n",
            "1. Q: What is hypertension?\n",
            "   A: Hypertension, or high blood pressure, is a common medical condition where blood pressure consistentl...\n",
            "\n",
            "2. Q: How is diabetes managed?\n",
            "   A: Diabetes management involves regular blood glucose monitoring, dietary modifications, physical activ...\n"
          ]
        }
      ],
      "source": [
        "# Create sample training data for fine-tuning\n",
        "def create_sample_training_data():\n",
        "    \"\"\"Create sample conversation data for fine-tuning\"\"\"\n",
        "    \n",
        "    training_data = [\n",
        "        {\n",
        "            \"question\": \"What is hypertension?\",\n",
        "            \"answer\": \"Hypertension, or high blood pressure, is a common medical condition where blood pressure consistently exceeds 130/80 mmHg. It affects millions worldwide and requires lifestyle modifications for management.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How is diabetes managed?\",\n",
        "            \"answer\": \"Diabetes management involves regular blood glucose monitoring, dietary modifications, physical activity, and medication as prescribed by healthcare providers. Type 2 diabetes is the most common form.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What makes a contract valid?\",\n",
        "            \"answer\": \"A valid contract requires five essential elements: offer, acceptance, consideration, capacity, and legality. All parties must have the legal ability to contract, and the contract's purpose must be legal.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is Python used for?\",\n",
        "            \"answer\": \"Python is widely used in web development, data science, machine learning, and automation. It's known for simplicity, readability, dynamic typing, and extensive standard library support.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are common machine learning algorithms?\",\n",
        "            \"answer\": \"Common machine learning algorithms include linear regression for predicting continuous values, decision trees for classification and regression, and various others like neural networks and support vector machines.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    return training_data\n",
        "\n",
        "# Create training data\n",
        "sample_training_data = create_sample_training_data()\n",
        "print(f\"‚úÖ Created {len(sample_training_data)} training examples\")\n",
        "print(\"Sample training data:\")\n",
        "for i, example in enumerate(sample_training_data[:2]):\n",
        "    print(f\"\\n{i+1}. Q: {example['question']}\")\n",
        "    print(f\"   A: {example['answer'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo"
      },
      "source": [
        "## üöÄ Demo: Fine-Tuning + RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "initialize_components"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Initializing components...\n",
            "üöÄ Initializing Configurable Document Bot...\n",
            "‚è≥ Loading embeddings model (first time downloads ~90MB)...\n",
            "‚úÖ Embeddings model loaded!\n",
            "\n",
            "‚úÖ Ollama LLM 'chatter' loaded!\n",
            "‚úÖ Components initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize both components\n",
        "print(\"üîß Initializing components...\")\n",
        "\n",
        "# Initialize fine-tuner\n",
        "fine_tuner = ModelFineTuner(config)\n",
        "\n",
        "# Initialize RAG bot\n",
        "rag_bot = ConfigurableDocBot(config)\n",
        "\n",
        "print(\"‚úÖ Components initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fine_tuning_demo"
      },
      "source": [
        "### Step 1: Fine-Tuning Demo (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "run_fine_tuning"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Starting fine-tuning demo...\n",
            "‚ö†Ô∏è This may take 10-30 minutes depending on your GPU...\n",
            "üîß Setting up model for fine-tuning...\n",
            "trainable params: 4,325,376 || all params: 359,148,544 || trainable%: 1.2043\n",
            "‚úÖ LoRA setup completed!\n",
            "‚úÖ Model setup completed!\n",
            "üìä Preparing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32772b137b634dd2bf22efd8e6ce27af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset prepared with 5 examples\n",
            "\n",
            "üí° Fine-tuning setup complete!\n",
            "   Uncomment the fine_tuner.fine_tune() line to actually run training.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to run fine-tuning (this may take a while)\n",
        "# Note: This is optional and requires significant computational resources\n",
        "\n",
        "print(\"üéØ Starting fine-tuning demo...\")\n",
        "print(\"‚ö†Ô∏è This may take 10-30 minutes depending on your GPU...\")\n",
        "\n",
        "# Setup model for fine-tuning\n",
        "fine_tuner.setup_model()\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = fine_tuner.prepare_dataset(sample_training_data)\n",
        "\n",
        "# Run fine-tuning (commented out by default to save time)\n",
        "# fine_tuner.fine_tune(dataset, \"./demo_fine_tuned_model\")\n",
        "\n",
        "print(\"\\nüí° Fine-tuning setup complete!\")\n",
        "print(\"   Uncomment the fine_tuner.fine_tune() line to actually run training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rag_demo"
      },
      "source": [
        "### Step 2: RAG Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "load_rag_domain"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Loading RAG domain...\n",
            "\n",
            "============================================================\n",
            "üìÇ Loading domain: medical\n",
            "üìÅ Document folder: ./Documents/Medical\n",
            "============================================================\n",
            "\n",
            "üìÑ Loading documents...\n",
            "  ‚úì Loaded guidelines.md\n",
            "\n",
            "üìö Total documents loaded: 1\n",
            "\n",
            "‚öôÔ∏è Processing documents...\n",
            "  ‚úì Split into 1 chunks\n",
            "  ‚è≥ Creating vector database...\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1814549361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load medical domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medical\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{config.DOCS_FOLDER}/Medical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2629038163.py\u001b[0m in \u001b[0;36mload_domain\u001b[0;34m(self, domain_name, docs_folder)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Process documents and create vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Setup QA chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2629038163.py\u001b[0m in \u001b[0;36m_process_documents\u001b[0;34m(self, documents, domain_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Create vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  ‚è≥ Creating vector database...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         self.vectorstore = Chroma.from_documents(\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             ):\n\u001b[0;32m-> 1365\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m   1366\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mids_with_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_empty_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                     self._collection.upsert(\n\u001b[0m\u001b[1;32m    652\u001b[0m                         \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_with_metadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    487\u001b[0m         )\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         self._client._upsert(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsert_request\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36m_upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_DATABASE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     ) -> bool:\n\u001b[0;32m--> 509\u001b[0;31m         return self.bindings.upsert(\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
          ]
        }
      ],
      "source": [
        "# Load a domain for RAG\n",
        "print(\"üìö Loading RAG domain...\")\n",
        "\n",
        "# Load medical domain\n",
        "success = rag_bot.load_domain(\"medical\", f\"{config.DOCS_FOLDER}/Medical\")\n",
        "\n",
        "if success:\n",
        "    print(\"‚úÖ Medical domain loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Failed to load medical domain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_rag_questions"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing RAG with sample questions...\n",
            "\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üîç Question: What is hypertension?\n",
            "üìÇ Domain: medical\n",
            "‚è≥ Searching documents...\n",
            "\n",
            "‚ùå Error: [Errno 111] Connection refused\n",
            "\n",
            "Q: What is hypertension?\n",
            "A: Error processing question: [Errno 111] Connection refused\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üîç Question: How is diabetes managed?\n",
            "üìÇ Domain: medical\n",
            "‚è≥ Searching documents...\n",
            "\n",
            "‚ùå Error: [Errno 111] Connection refused\n",
            "\n",
            "Q: How is diabetes managed?\n",
            "A: Error processing question: [Errno 111] Connection refused\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üîç Question: What are the treatment recommendations for high blood pressure?\n",
            "üìÇ Domain: medical\n",
            "‚è≥ Searching documents...\n",
            "\n",
            "‚ùå Error: [Errno 111] Connection refused\n",
            "\n",
            "Q: What are the treatment recommendations for high blood pressure?\n",
            "A: Error processing question: [Errno 111] Connection refused\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test RAG with sample questions\n",
        "test_questions = [\n",
        "    \"What is hypertension?\",\n",
        "    \"How is diabetes managed?\",\n",
        "    \"What are the treatment recommendations for high blood pressure?\"\n",
        "]\n",
        "\n",
        "print(\"üîç Testing RAG with sample questions...\\n\")\n",
        "\n",
        "for question in test_questions:\n",
        "    answer = rag_bot.ask(question)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive_chat"
      },
      "source": [
        "### Step 3: Interactive Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_demo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí° Uncomment interactive_chat() call to start interactive mode\n",
            "   Or run individual questions using: rag_bot.ask('your question')\n"
          ]
        }
      ],
      "source": [
        "# Interactive chat demo\n",
        "def interactive_chat():\n",
        "    \"\"\"Run interactive chat with the RAG bot\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  üí¨ INTERACTIVE CHAT DEMO\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Commands:\")\n",
        "    print(\"  ‚Ä¢ Type your question to ask\")\n",
        "    print(\"  ‚Ä¢ 'switch <domain>' - change domain (medical/legal/tech)\")\n",
        "    print(\"  ‚Ä¢ 'quit' - exit chat\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(f\"[{rag_bot.current_domain}] You: \").strip()\n",
        "            \n",
        "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\\nüëã Goodbye!\\n\")\n",
        "                break\n",
        "            \n",
        "            if user_input.lower().startswith('switch'):\n",
        "                parts = user_input.split()\n",
        "                if len(parts) >= 2:\n",
        "                    new_domain = parts[1]\n",
        "                    docs_path = f\"{config.DOCS_FOLDER}/{new_domain.capitalize()}\"\n",
        "                    \n",
        "                    if rag_bot.switch_domain(new_domain, docs_path):\n",
        "                        print(f\"‚úÖ Switched to {new_domain} domain\")\n",
        "                    else:\n",
        "                        print(f\"‚ùå Failed to switch to {new_domain} domain\")\n",
        "                else:\n",
        "                    print(\"Usage: switch <domain> (medical/legal/tech)\")\n",
        "                continue\n",
        "            \n",
        "            if not user_input:\n",
        "                continue\n",
        "            \n",
        "            # Ask the question\n",
        "            answer = rag_bot.ask(user_input)\n",
        "            print(f\"\\nü§ñ Assistant:\\n{answer}\\n\")\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Goodbye!\\n\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error: {str(e)}\\n\")\n",
        "\n",
        "# Run interactive chat (uncomment to use)\n",
        "# interactive_chat()\n",
        "\n",
        "print(\"üí° Uncomment interactive_chat() call to start interactive mode\")\n",
        "print(\"   Or run individual questions using: rag_bot.ask('your question')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_features"
      },
      "source": [
        "## üîß Advanced Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "domain_switching"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Demonstrating domain switching...\n",
            "\n",
            "\n",
            "--- Switching to MEDICAL domain ---\n",
            "\n",
            "üîÑ Switching from 'medical' to 'medical'...\n",
            "\n",
            "============================================================\n",
            "üìÇ Loading domain: medical\n",
            "üìÅ Document folder: ./Documents/Medical\n",
            "============================================================\n",
            "üóëÔ∏è Removing old vector database...\n",
            "\n",
            "üìÑ Loading documents...\n",
            "  ‚úì Loaded guidelines.md\n",
            "\n",
            "üìö Total documents loaded: 1\n",
            "\n",
            "‚öôÔ∏è Processing documents...\n",
            "  ‚úì Split into 1 chunks\n",
            "  ‚è≥ Creating vector database...\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2733129031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdocs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{config.DOCS_FOLDER}/{domain_name.capitalize()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mrag_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer[:200]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2629038163.py\u001b[0m in \u001b[0;36mswitch_domain\u001b[0;34m(self, domain_name, docs_folder)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34m\"\"\"Switch to a different domain/customer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüîÑ Switching from '{self.current_domain}' to '{domain_name}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ RAG component defined!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2629038163.py\u001b[0m in \u001b[0;36mload_domain\u001b[0;34m(self, domain_name, docs_folder)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Process documents and create vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Setup QA chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2629038163.py\u001b[0m in \u001b[0;36m_process_documents\u001b[0;34m(self, documents, domain_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Create vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  ‚è≥ Creating vector database...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         self.vectorstore = Chroma.from_documents(\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             ):\n\u001b[0;32m-> 1365\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m   1366\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mids_with_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_empty_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                     self._collection.upsert(\n\u001b[0m\u001b[1;32m    652\u001b[0m                         \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_with_metadatas\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    487\u001b[0m         )\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         self._client._upsert(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsert_request\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36m_upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_DATABASE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     ) -> bool:\n\u001b[0;32m--> 509\u001b[0;31m         return self.bindings.upsert(\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
          ]
        }
      ],
      "source": [
        "# Demonstrate domain switching\n",
        "print(\"üîÑ Demonstrating domain switching...\\n\")\n",
        "\n",
        "# Test different domains\n",
        "domains = [\n",
        "    (\"medical\", \"What is hypertension?\"),\n",
        "    (\"legal\", \"What makes a contract valid?\"),\n",
        "    (\"tech\", \"What is Python used for?\")\n",
        "]\n",
        "\n",
        "for domain_name, question in domains:\n",
        "    print(f\"\\n--- Switching to {domain_name.upper()} domain ---\")\n",
        "    \n",
        "    docs_path = f\"{config.DOCS_FOLDER}/{domain_name.capitalize()}\"\n",
        "    \n",
        "    if rag_bot.switch_domain(domain_name, docs_path):\n",
        "        answer = rag_bot.ask(question)\n",
        "        print(f\"Answer: {answer[:200]}...\")\n",
        "    else:\n",
        "        print(f\"Failed to load {domain_name} domain\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_prompts"
      },
      "outputs": [],
      "source": [
        "# Custom prompt engineering\n",
        "def create_custom_prompt_template(domain_type: str) -> str:\n",
        "    \"\"\"Create domain-specific prompt templates\"\"\"\n",
        "    \n",
        "    templates = {\n",
        "        \"medical\": \"\"\"\n",
        "CONTEXT FROM MEDICAL DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "MEDICAL QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Provide accurate medical information based on the context\n",
        "2. Include relevant medical terminology\n",
        "3. Suggest consulting healthcare professionals for personal medical advice\n",
        "4. If information is not available, recommend consulting a medical professional\n",
        "\n",
        "MEDICAL ANSWER:\n",
        "\"\"\",\n",
        "        \n",
        "        \"legal\": \"\"\"\n",
        "CONTEXT FROM LEGAL DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "LEGAL QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Provide legal information based on the context\n",
        "2. Use precise legal terminology\n",
        "3. Include relevant legal principles and precedents if mentioned\n",
        "4. Always recommend consulting qualified legal counsel for specific legal advice\n",
        "\n",
        "LEGAL ANSWER:\n",
        "\"\"\",\n",
        "        \n",
        "        \"tech\": \"\"\"\n",
        "CONTEXT FROM TECHNICAL DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "TECHNICAL QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Provide technical information based on the context\n",
        "2. Include relevant technical details and code examples if available\n",
        "3. Explain technical concepts clearly\n",
        "4. Suggest additional resources or documentation when helpful\n",
        "\n",
        "TECHNICAL ANSWER:\n",
        "\"\"\"\n",
        "    }\n",
        "    \n",
        "    return templates.get(domain_type, templates[\"tech\"])\n",
        "\n",
        "# Demonstrate custom prompts\n",
        "print(\"üé® Demonstrating custom prompt templates...\\n\")\n",
        "\n",
        "for domain in [\"medical\", \"legal\", \"tech\"]:\n",
        "    template = create_custom_prompt_template(domain)\n",
        "    print(f\"{domain.upper()} Prompt Template:\")\n",
        "    print(template[:300] + \"...\\n\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_models"
      },
      "source": [
        "## üì¶ Export and Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_components"
      },
      "outputs": [],
      "source": [
        "# Save components for later use\n",
        "def save_components():\n",
        "    \"\"\"Save trained components and configurations\"\"\"\n",
        "    \n",
        "    # Create export directory\n",
        "    export_dir = Path(\"./exported_components\")\n",
        "    export_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save configuration\n",
        "    config_dict = {\n",
        "        \"BASE_MODEL\": config.BASE_MODEL,\n",
        "        \"OLLAMA_MODEL\": config.OLLAMA_MODEL,\n",
        "        \"EMBEDDING_MODEL\": config.EMBEDDING_MODEL,\n",
        "        \"CHUNK_SIZE\": config.CHUNK_SIZE,\n",
        "        \"CHUNK_OVERLAP\": config.CHUNK_OVERLAP,\n",
        "        \"RETRIEVAL_K\": config.RETRIEVAL_K,\n",
        "        \"USE_LORA\": config.USE_LORA,\n",
        "        \"LORA_R\": config.LORA_R,\n",
        "        \"LORA_ALPHA\": config.LORA_ALPHA,\n",
        "    }\n",
        "    \n",
        "    with open(export_dir / \"config.json\", \"w\") as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "    \n",
        "    # Save sample training data\n",
        "    with open(export_dir / \"training_data.json\", \"w\") as f:\n",
        "        json.dump(sample_training_data, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ Components saved to {export_dir}\")\n",
        "    print(f\"   - Configuration: config.json\")\n",
        "    print(f\"   - Training data: training_data.json\")\n",
        "    print(f\"   - Vector databases: {config.CHROMA_DB_PATH}_*\")\n",
        "\n",
        "# Save components\n",
        "save_components()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üìã Summary\n",
        "\n",
        "This notebook provides a complete fine-tuning and RAG system with:\n",
        "\n",
        "### ‚úÖ Features Implemented:\n",
        "- **Fine-tuning Component**: Custom model training with LoRA support\n",
        "- **RAG Component**: Document retrieval and question answering\n",
        "- **Domain Switching**: Multiple document domains (Medical, Legal, Tech)\n",
        "- **Interactive Chat**: Real-time Q&A interface\n",
        "- **Sample Data**: Pre-configured documents and training data\n",
        "\n",
        "### üéØ Use Cases:\n",
        "1. **Customer Support Bots**: Fine-tune on company-specific conversations\n",
        "2. **Domain Experts**: Load specialized documents for professional Q&A\n",
        "3. **Educational Tools**: Create tutoring systems for specific subjects\n",
        "4. **Research Assistants**: Load academic papers for research Q&A\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. **Upload Your Documents**: Replace sample docs with your own files\n",
        "2. **Prepare Training Data**: Create domain-specific conversation data\n",
        "3. **Run Fine-Tuning**: Train custom models for your use case\n",
        "4. **Deploy**: Export and deploy to production environments\n",
        "\n",
        "### üí° Tips:\n",
        "- Use GPU runtime for faster fine-tuning\n",
        "- Start with small datasets for testing\n",
        "- Monitor training loss to avoid overfitting\n",
        "- Experiment with different chunk sizes for better retrieval\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Congratulations!** You now have a fully configurable fine-tuning and RAG system ready for deployment!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
