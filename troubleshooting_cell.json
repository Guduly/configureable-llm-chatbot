{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## ðŸ”§ Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "#### 1. **ChromaDB Read-Only Error**\n",
        "- **Issue**: `attempt to write a readonly database`\n",
        "- **Solution**: The notebook now automatically falls back to in-memory database\n",
        "\n",
        "#### 2. **Ollama Connection Refused**\n",
        "- **Issue**: `[Errno 111] Connection refused`\n",
        "- **Solution**: Ollama is not available in Colab. Use a local environment or replace with HuggingFace models\n",
        "\n",
        "#### 3. **Missing Dependencies**\n",
        "- **Issue**: Import errors for langchain packages\n",
        "- **Solution**: Run the installation cell again. Some packages may require restart.\n",
        "\n",
        "#### 4. **GPU Memory Issues**\n",
        "- **Issue**: Out of memory errors during fine-tuning\n",
        "- **Solution**: Reduce batch size, use smaller model, or enable more aggressive quantization\n",
        "\n",
        "#### 5. **Long Training Times**\n",
        "- **Issue**: Fine-tuning taking too long\n",
        "- **Solution**: Use fewer training examples, reduce epochs, or use a smaller base model\n",
        "\n",
        "### Quick Fixes:\n",
        "\n",
        "```python\n",
        "# For Ollama issues, use HuggingFace instead:\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"microsoft/DialoGPT-medium\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "# For memory issues, reduce batch size:\n",
        "config.BATCH_SIZE = 2  # Reduce from 4\n",
        "config.MAX_LENGTH = 256  # Reduce from 512\n",
        "```\n",
        "\n",
        "### Testing Without Ollama:\n",
        "If Ollama is not available, you can test the RAG system by:\n",
        "1. Using HuggingFace models instead\n",
        "2. Testing document loading and vector storage separately\n",
        "3. Running fine-tuning component independently\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **Note**: This notebook is designed to work in various environments. In Colab, some features may need adjustments due to platform limitations.\n",
        "\n",
        "### Debug Commands:\n",
        "\n",
        "```python\n",
        "# Check if Ollama is available\n",
        "import subprocess\n",
        "try:\n",
        "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "    print('Ollama available:', result.returncode == 0)\n",
        "except:\n",
        "    print('Ollama not available')\n",
        "\n",
        "# Test embeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "test_embedding = embeddings.embed_query(\"test\")\n",
        "print('Embeddings working:', len(test_embedding) > 0)\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU device:', torch.cuda.get_device_name(0))\n",
        "```\n",
        "\n",
        "### Environment-Specific Notes:\n",
        "\n",
        "**Google Colab:**\n",
        "- Ollama is not available â†’ Use HuggingFace models\n",
        "- Persistent storage is limited â†’ Use in-memory databases\n",
        "- GPU timeouts may occur â†’ Save checkpoints frequently\n",
        "\n",
        "**Local Environment:**\n",
        "- Full Ollama support available\n",
        "- Persistent databases work normally\n",
        "- No GPU time limits\n",
        "\n",
        "**Production Deployment:**\n",
        "- Use persistent vector databases (Pinecone, Weaviate, etc.)\n",
        "- Consider API-based LLM services for scalability\n",
        "- Implement proper error handling and monitoring\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}